{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5511eda0",
   "metadata": {},
   "source": [
    "# MotherTongue vs Spoken Language\n",
    "Riconoscimento tramite video della lingua e vedere se questa lingua è la lingua madre di una persona o una semplice lingua parlata. \\\n",
    "Il metodo utilizzato è attraverso la Knowledge Distillation, approccio che vede l'utilizzo di un Teacher che impara la distinzione tra i due tipi di lingua con supporto audio-video, questa sua conoscenza la poi 'distilla' ad un suo Student, che tramite determinati approcci (embeddings) imparati dal Teacher prova a risolvere la stessa task solamente vedendo il labiale di una persona, quindi solamente con supporto video SENZA audio.\n",
    "\n",
    "\n",
    "\n",
    "https://colab.research.google.com/drive/18LNEeYOwP_03wEUgJWYwfV0_fz95I4d1?usp=sharing - based on this code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719faca",
   "metadata": {},
   "source": [
    "## STEP 1: Fattibilità del problema\n",
    "Il modello Teacher, in questo caso Whisper-small, è in grado di poter classificare lingua madre da lingua parlata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d302bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creato CSV con 3373 righe in /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/EMODB/dataset.csv\n",
      "Dataset({\n",
      "    features: ['audio_path', 'video_path', 'label'],\n",
      "    num_rows: 3373\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# creazione del dataset multimodale, estrazione dell'audio e csv associato con i path dei vari video\n",
    "import glob\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from datasets import Dataset, Audio\n",
    "\n",
    "dataset_dir    = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/EMODB\"\n",
    "audio_dir      = os.path.join(dataset_dir, \"audio_wav\")\n",
    "csv_path       = os.path.join(dataset_dir, \"dataset.csv\")\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "# raccolta dei video nella cartella dataset\n",
    "video_paths = []\n",
    "for ext in (\"*.mp4\", \"*.MP4\", \"*.mov\", \"*.MOV\"):\n",
    "    video_paths.extend(glob.glob(os.path.join(dataset_dir, \"**\", ext), recursive=True))\n",
    "\n",
    "rows = []\n",
    "for vp in video_paths:\n",
    "    fname = os.path.basename(vp)\n",
    "    label = 1 if \"EN\" in fname.upper() else 0\n",
    "\n",
    "    # costruzione per i file audio\n",
    "    base_no_ext = os.path.splitext(fname)[0]\n",
    "    wav_path    = os.path.join(audio_dir, base_no_ext + \".wav\")\n",
    "\n",
    "    # estrazione audio con la funzione ffmpeg\n",
    "    if not os.path.exists(wav_path):\n",
    "        subprocess.run([\n",
    "            \"ffmpeg\", \"-i\", vp,\n",
    "            \"-ar\", \"16000\", \"-ac\", \"1\",    \n",
    "            \"-f\", \"wav\", \"-vn\", wav_path \n",
    "        ], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "    rows.append({\n",
    "        \"audio_path\": wav_path,\n",
    "        \"video_path\": vp,\n",
    "        \"label\":      label\n",
    "    })\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Creato CSV con {len(df)} righe in {csv_path}\")\n",
    "\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "ds = ds.cast_column(\"audio_path\", Audio(sampling_rate=16_000))\n",
    "\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27dfb917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22c34e88d434c3a9e7ac70326dea425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4161c6cc46134ae59a400d72a25543fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/3373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54b804bfbeb24fefacc6d1901b3097d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/3373 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fine tuning di Whisper\n",
    "# os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "import evaluate\n",
    "\n",
    "\n",
    "ds = load_dataset(\"csv\", data_files=\"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/EMODB/dataset.csv\", split=\"train\")\n",
    "ds = ds.class_encode_column(\"label\")\n",
    "\n",
    "split_ds = ds.train_test_split(\n",
    "    test_size=0.3,\n",
    "    seed=42,\n",
    "    stratify_by_column='label',\n",
    ")\n",
    "train_ds = split_ds[\"train\"]\n",
    "test_ds = split_ds[\"test\"]\n",
    "\n",
    "\n",
    "#imposta la colonna audio come waveform a 16 kHz\n",
    "train_ds = train_ds.cast_column(\"audio_path\", Audio(sampling_rate=16_000))\n",
    "test_ds  = test_ds.cast_column(\"audio_path\",  Audio(sampling_rate=16_000))\n",
    "\n",
    "#print(train_ds[0])\n",
    "#print(test_ds[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf05b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device active: mps\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abcd1fe14ddc4e20b917b0281d5027f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2361 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9394d732f940ab9284364254175a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1012 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='74' max='365' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 74/365 03:00 < 12:08, 0.40 it/s, Epoch 0.99/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='506' max='506' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [506/506 01:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'recall_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 139\u001b[0m\n\u001b[1;32m    114\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    115\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./whisper_cls_tiny_lora_mps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m     per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,    \u001b[38;5;66;03m# ↓ batch 2 per contenere VRAM\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,                       \u001b[38;5;66;03m# fp16 non usato ma input è half\u001b[39;00m\n\u001b[1;32m    128\u001b[0m )\n\u001b[1;32m    130\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m    131\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    132\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 139\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#stampa metriche e confusion matrix\u001b[39;00m\n\u001b[1;32m    142\u001b[0m metrics \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mevaluate(test_ds)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   2250\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:2661\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2658\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2660\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_log_save_evaluate(\n\u001b[1;32m   2662\u001b[0m     tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate\n\u001b[1;32m   2663\u001b[0m )\n\u001b[1;32m   2665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2666\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2667\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:3096\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   3094\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 3096\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate(trial, ignore_keys_for_eval)\n\u001b[1;32m   3097\u001b[0m     is_new_best_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_determine_best_metric(metrics\u001b[38;5;241m=\u001b[39mmetrics, trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m   3099\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39msave_strategy \u001b[38;5;241m==\u001b[39m SaveStrategy\u001b[38;5;241m.\u001b[39mBEST:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 3045\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys_for_eval)\n\u001b[1;32m   3046\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   3048\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:4154\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4151\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   4153\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 4154\u001b[0m output \u001b[38;5;241m=\u001b[39m eval_loop(\n\u001b[1;32m   4155\u001b[0m     eval_dataloader,\n\u001b[1;32m   4156\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4157\u001b[0m     \u001b[38;5;66;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;00m\n\u001b[1;32m   4158\u001b[0m     \u001b[38;5;66;03m# self.args.prediction_loss_only\u001b[39;00m\n\u001b[1;32m   4159\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   4160\u001b[0m     ignore_keys\u001b[38;5;241m=\u001b[39mignore_keys,\n\u001b[1;32m   4161\u001b[0m     metric_key_prefix\u001b[38;5;241m=\u001b[39mmetric_key_prefix,\n\u001b[1;32m   4162\u001b[0m )\n\u001b[1;32m   4164\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   4165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/transformers/trainer.py:4443\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4441\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlosses\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_losses \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   4442\u001b[0m     eval_set_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m all_inputs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 4443\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   4444\u001b[0m         EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39meval_set_kwargs)\n\u001b[1;32m   4445\u001b[0m     )\n\u001b[1;32m   4446\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4447\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[0;32mIn[23], line 107\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(eval_pred)\u001b[0m\n\u001b[1;32m    102\u001b[0m logits, labels \u001b[38;5;241m=\u001b[39m eval_pred\n\u001b[1;32m    103\u001b[0m preds \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39margmax(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m:       f1_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: recall_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: precision_metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mpreds, references\u001b[38;5;241m=\u001b[39mlabels)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    109\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recall_metric' is not defined"
     ]
    }
   ],
   "source": [
    "import types\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "from transformers import (\n",
    "    WhisperProcessor,\n",
    "    WhisperForConditionalGeneration,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"   # evita override fp16\n",
    "os.environ.pop(\"USE_FP16\", None)\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device active: {device}\")\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "MODEL_NAME = \"openai/whisper-tiny\"   # usa tiny/base per ridurre VRAM\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n",
    "model     = WhisperForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()  # taglia VRAM a costo di +latency\n",
    "\n",
    "model.config.forced_decoder_ids = None\n",
    "model.config.suppress_tokens    = []\n",
    "model.classifier = torch.nn.Linear(model.config.d_model, 2)\n",
    "\n",
    "#forward cambiato da trascrizione a classificazione\n",
    "\n",
    "def forward_with_classification(self, input_features=None, labels=None, **kwargs):\n",
    "    if input_features is None:\n",
    "        raise ValueError(\"input_features cannot be None\")\n",
    "    enc_out = self.model.encoder(input_features)[0]\n",
    "    pooled  = enc_out.mean(dim=1)\n",
    "    logits  = self.classifier(pooled)\n",
    "    loss = torch.nn.functional.cross_entropy(logits, labels) if labels is not None else None\n",
    "    return {\"loss\": loss, \"logits\": logits}\n",
    "\n",
    "model.forward = types.MethodType(forward_with_classification, model)\n",
    "\n",
    "for p in model.model.encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 5) LoRA per ridurre la computazione\n",
    "# --------------------------------------------------\n",
    "peft_cfg = LoraConfig(\n",
    "    task_type      = TaskType.SEQ_CLS,\n",
    "    inference_mode = False,\n",
    "    r              = 8,\n",
    "    lora_alpha     = 16,\n",
    "    lora_dropout   = 0.05,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"],\n",
    "    bias           = \"none\",\n",
    ")\n",
    "model = get_peft_model(model, peft_cfg)\n",
    "model.to(device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 6) Dataset preprocessing (definisci train_ds/test_ds)\n",
    "# --------------------------------------------------\n",
    "\n",
    "def preprocess(batch):\n",
    "    audio = batch[\"audio_path\"][\"array\"]\n",
    "    feats = processor.feature_extractor(audio, sampling_rate=16_000, return_tensors=\"pt\")\n",
    "    batch[\"input_features\"] = feats.input_features[0]  # (80, F)\n",
    "    batch[\"labels\"]         = batch[\"label\"]\n",
    "    return batch\n",
    "\n",
    "train_ds = train_ds.map(preprocess, remove_columns=train_ds.column_names, num_proc=4)\n",
    "test_ds  = test_ds.map(preprocess,  remove_columns=test_ds.column_names,  num_proc=4)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 7) Data collator\n",
    "# --------------------------------------------------\n",
    "@dataclass\n",
    "class DataCollatorWhisperCls:\n",
    "    dtype = torch.float32  # for MPS evita mismatch Half vs Float\n",
    "    def __call__(self, features):\n",
    "        tensors = [torch.tensor(f[\"input_features\"], dtype=torch.float32) for f in features]\n",
    "        max_len = max(t.shape[-1] for t in tensors)\n",
    "        padded  = [torch.nn.functional.pad(t, (0, max_len - t.shape[-1])) for t in tensors]\n",
    "        input_features = torch.stack(padded)\n",
    "        labels         = torch.tensor([f[\"labels\"] for f in features], dtype=torch.long)\n",
    "        return {\"input_features\": input_features, \"labels\": labels}\n",
    "\n",
    "data_collator = DataCollatorWhisperCls()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 8) Metriche\n",
    "# --------------------------------------------------\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric       = evaluate.load(\"f1\")\n",
    "recall_metric       = evaluate.load(\"recall\")\n",
    "precision_metric      = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\":       f1_metric.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"recall\": recall_metric.compute(predictions=preds, references=labels)[\"recall\"],\n",
    "        \"precision\": precision_metric.compute(predictions=preds, references=labels)[\"precision\"],\n",
    "    }\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 9) TrainingArguments (batch minimale, accumulo alto)\n",
    "# --------------------------------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./whisper_cls_tiny_lora_mps\",\n",
    "    per_device_train_batch_size=2,    # ↓ batch 2 per contenere VRAM\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=16,   # eff. batch 32\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    fp16=False,                       # fp16 non usato ma input è half\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "#stampa metriche e confusion matrix\n",
    "metrics = trainer.evaluate(test_ds)\n",
    "print(\"Metriche calcolate\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:>10}: {v:.4f}\")\n",
    "\n",
    "\n",
    "pred_out = trainer.predict(test_ds)\n",
    "y_true   = pred_out.label_ids\n",
    "y_pred   = pred_out.predictions.argmax(axis=-1)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c249aa",
   "metadata": {},
   "source": [
    "L'accuracy sembra comunque essere buona, ma il numero di falsi negativi vede una difficoltà a capire se la lingua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0045b101",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08c3288a",
   "metadata": {},
   "source": [
    "## Teacher: Whisper-medium di OpenAI - Student: AV-HuBERT (Large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
