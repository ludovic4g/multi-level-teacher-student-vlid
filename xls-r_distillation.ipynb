{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68d267e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login()\n",
    "\n",
    "#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"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813f245",
   "metadata": {},
   "source": [
    "# XLS-R DI FACEBOOK/META PER IL RICONOSCIMENTO DELLE LINGUE\n",
    "Approccio Wave2Vec per il riconoscimento di tale tramite le onde dell'audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1d2a5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_7_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_2_50.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_2_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_9_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_6_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_8_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_7_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_8_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_4_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_2_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_7_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_6_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_7_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_8_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_2_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_6_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_3_25.mp4\n"
     ]
    }
   ],
   "source": [
    "# creazione del train e test dataset\n",
    "# creare csv con file path e path audio corrispondente\n",
    "# INGLESE = 0, ITALIANO = 1, SPAGNOLO = 2 (senza fr per adesso)\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "os.environ.pop(\"USE_FP16\", None)\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "dataset_dir = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE\"\n",
    "audio_dir   = os.path.join(dataset_dir, \"audio_wav\")\n",
    "csv_path    = os.path.join(dataset_dir, \"dataset.csv\")\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "video_paths = glob.glob(os.path.join(dataset_dir, \"**/*.mp4\"), recursive=True)\n",
    "\n",
    "rows = []\n",
    "for vp in video_paths:\n",
    "    #low = vp.lower()\n",
    "    if   \"English\" in vp: label = 0\n",
    "    elif \"Italian\" in vp: label = 1\n",
    "    #elif \"French\"  in vp: label = 3\n",
    "    elif \"Spanish\" in vp: label = 2\n",
    "    else:\n",
    "        print(\"Lingua non riconosciuta → salto:\", vp)\n",
    "        continue\n",
    "\n",
    "    fname = os.path.splitext(os.path.basename(vp))[0]\n",
    "    wav   = os.path.join(audio_dir, f\"{fname}_{label}language.wav\")\n",
    "\n",
    "    if not os.path.exists(wav):\n",
    "        res = subprocess.run(\n",
    "            [\"ffmpeg\", \"-i\", vp, \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "             \"-f\", \"wav\", \"-vn\", wav],\n",
    "            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        if res.returncode != 0:\n",
    "            print(\"ffmpeg errore → salto:\", vp)\n",
    "            continue        # non appendere se fallita conversione\n",
    "\n",
    "    rows.append({\"audio_path\": wav, \"video_path\": vp, \"label\": label})\n",
    "\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "    print(f\"CSV creato: {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "83c3f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caricamente train e test\n",
    "train_ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/dataset.csv\", split=\"train\"\n",
    ")\n",
    "\n",
    "test_ds = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files=\"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/EMODB/dataset.csv\", split=\"train\"\n",
    ")\n",
    "\n",
    "#print(test_full)\n",
    "# ridurre dataset test\n",
    "#indices = []\n",
    "#for lab in set(test_full['label']):\n",
    " #   idx = [i for i, l in enumerate(test_full[\"label\"]) if l == lab][:20]\n",
    " #   indices.extend(idx)\n",
    "  #  print(indices)\n",
    "\n",
    "#test_ds = test_full.select(indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3dc10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3470c211308941919b3bd3ac632a00c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/95 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b62aec8d59bb49aa802e02006576432e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 71\u001b[0m\n\u001b[1;32m     65\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForAudioClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     66\u001b[0m     FE_MODEL,\n\u001b[1;32m     67\u001b[0m     num_labels\u001b[38;5;241m=\u001b[39mNUM_LABELS,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# ---------------- trainer setup\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     72\u001b[0m     output_dir                   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls_r_trial_1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     73\u001b[0m     evaluation_strategy          \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     74\u001b[0m     save_strategy                \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     75\u001b[0m     learning_rate                \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m,\n\u001b[1;32m     76\u001b[0m     per_device_train_batch_size  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     77\u001b[0m     gradient_accumulation_steps  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m     78\u001b[0m     per_device_eval_batch_size   \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     79\u001b[0m     num_train_epochs             \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     80\u001b[0m     warmup_ratio                 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m,\n\u001b[1;32m     81\u001b[0m     logging_steps                \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     82\u001b[0m     load_best_model_at_end       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     83\u001b[0m     metric_for_best_model        \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     84\u001b[0m     report_to                    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m )\n\u001b[1;32m     87\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorWithPadding(feature_extractor, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     89\u001b[0m accuracy  \u001b[38;5;241m=\u001b[39m evaluate\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'"
     ]
    }
   ],
   "source": [
    "# XLS-R fine-tuning – end-to-end\n",
    "import numpy as np\n",
    "from datasets import Audio\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoFeatureExtractor,\n",
    "    AutoModelForAudioClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "\n",
    "# ---------------- parametri base\n",
    "FE_MODEL   = \"facebook/wav2vec2-base\"   # backbone multilingue\n",
    "SR         = 16_000\n",
    "CHUNK_SEC  = 5\n",
    "STRIDE_SEC = 2.5\n",
    "MAX_SEG    = 8\n",
    "NUM_LABELS = 4\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(FE_MODEL)\n",
    "\n",
    "# ---------------- helper: assegna id clip\n",
    "def add_clip_id(ex, idx):\n",
    "    ex[\"clip_id\"] = idx\n",
    "    return ex\n",
    "\n",
    "train_ds = train_ds.map(add_clip_id, with_indices=True)\n",
    "test_ds  = test_ds.map(add_clip_id,  with_indices=True)\n",
    "\n",
    "# ---------------- cast colonna audio\n",
    "train_ds = train_ds.cast_column(\"audio_path\", Audio(sampling_rate=SR))\n",
    "test_ds  = test_ds.cast_column(\"audio_path\",  Audio(sampling_rate=SR))\n",
    "\n",
    "# ---------------- funzione di segmentazione (un record in output = un segmento)\n",
    "def segment_encode(ex):\n",
    "    wav = ex[\"audio_path\"][\"array\"]\n",
    "    sr  = ex[\"audio_path\"][\"sampling_rate\"]\n",
    "    chunk  = int(CHUNK_SEC  * sr)\n",
    "    stride = int(STRIDE_SEC * sr)\n",
    "\n",
    "    segs = [wav[s:s+chunk] for s in range(0, len(wav)-chunk+1, stride)][:MAX_SEG]\n",
    "    if not segs:                               # clip più corto di 5 s\n",
    "        segs = [np.pad(wav, (0, chunk-len(wav)))]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        segs,\n",
    "        sampling_rate=sr,\n",
    "        max_length=chunk,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )[\"input_values\"]\n",
    "\n",
    "    # genera una lista di dict → HF la “flattena” in record separati\n",
    "    return {\n",
    "        \"input_values\": inputs,\n",
    "        \"labels\":       [ex[\"label\"]]*len(inputs),\n",
    "        \"clip_id\":      [ex[\"clip_id\"]]*len(inputs),\n",
    "    }\n",
    "\n",
    "train_ds = train_ds.map(segment_encode, batched=False, remove_columns=train_ds.column_names)\n",
    "test_ds  = test_ds.map(segment_encode,  batched=False, remove_columns=test_ds.column_names)\n",
    "\n",
    "# ---------------- modello\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    FE_MODEL,\n",
    "    num_labels=NUM_LABELS,\n",
    ")\n",
    "\n",
    "# ---------------- trainer setup\n",
    "training_args = TrainingArguments(\n",
    "    output_dir                   = \"xls_r_trial_1\",\n",
    "    eval_strategy          = \"epoch\",\n",
    "    save_strategy                = \"epoch\",\n",
    "    learning_rate                = 3e-5,\n",
    "    per_device_train_batch_size  = 32,\n",
    "    gradient_accumulation_steps  = 4,\n",
    "    per_device_eval_batch_size   = 32,\n",
    "    num_train_epochs             = 10,\n",
    "    warmup_ratio                 = 0.1,\n",
    "    logging_steps                = 10,\n",
    "    load_best_model_at_end       = True,\n",
    "    metric_for_best_model        = \"accuracy\",\n",
    "    report_to                    = \"none\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorWithPadding(feature_extractor, padding=True)\n",
    "\n",
    "accuracy  = evaluate.load(\"accuracy\")\n",
    "f1        = evaluate.load(\"f1\")\n",
    "recall    = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    return {\n",
    "        \"accuracy\":  accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"f1\":        f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"],\n",
    "        \"recall\":    recall.compute(predictions=preds, references=labels, average=\"weighted\")[\"recall\"],\n",
    "        \"precision\": precision.compute(predictions=preds, references=labels, average=\"weighted\")[\"precision\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model           = model,\n",
    "    args            = training_args,\n",
    "    train_dataset   = train_ds,\n",
    "    eval_dataset    = test_ds,\n",
    "    data_collator   = data_collator,\n",
    "    compute_metrics = compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37adde98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "preds = trainer.predict(test_ds)\n",
    "\n",
    "y_true   = preds.label_ids\n",
    "y_pred   = preds.predictions.argmax(axis=-1)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3257576",
   "metadata": {},
   "source": [
    "inferenza su un video a caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d39ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codice con video /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/EMODB/audio_wav/003_f_021_1_6_IT_4.wav"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
