{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "169f0542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_7_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_2_50.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_2_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_9_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_6_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_8_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_7_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_8_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_4_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_2_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_7_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_3_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_6_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_1_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_7_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_8_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_4_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_1_2_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_2_6_30.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_1_1_5_25.mp4\n",
      "Lingua non riconosciuta → salto: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/French/8_2_2_3_25.mp4\n"
     ]
    }
   ],
   "source": [
    "# creare csv con file path e path audio corrispondente CON BABELE\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"ACCELERATE_MIXED_PRECISION\"] = \"no\"\n",
    "os.environ.pop(\"USE_FP16\", None)\n",
    "os.environ.setdefault(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"0.0\")\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "dataset_dir = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE\"\n",
    "audio_dir   = os.path.join(dataset_dir, \"audio_wav\")\n",
    "csv_path    = os.path.join(dataset_dir, \"dataset.csv\")\n",
    "os.makedirs(audio_dir, exist_ok=True)\n",
    "\n",
    "video_paths = glob.glob(os.path.join(dataset_dir, \"**/*.mp4\"), recursive=True)\n",
    "\n",
    "rows = []\n",
    "for vp in video_paths:\n",
    "    if   \"English\" in vp: label = 0\n",
    "    elif \"Italian\" in vp: label = 1\n",
    "    #elif \"french\"  in vp: label = 3\n",
    "    elif \"Spanish\" in vp: label = 2\n",
    "    else:\n",
    "        print(\"Lingua non riconosciuta → salto:\", vp)\n",
    "        continue\n",
    "\n",
    "    fname = os.path.splitext(os.path.basename(vp))[0]\n",
    "    wav   = os.path.join(audio_dir, f\"{fname}{label}language.wav\")\n",
    "\n",
    "    if not os.path.exists(wav):\n",
    "        res = subprocess.run(\n",
    "            [\"ffmpeg\", \"-i\", vp, \"-ar\", \"16000\", \"-ac\", \"1\",\n",
    "             \"-f\", \"wav\", \"-vn\", wav],\n",
    "            stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "        if res.returncode != 0:\n",
    "            print(\"ffmpeg errore → salto:\", vp)\n",
    "            continue        # non appendere se fallita conversione\n",
    "\n",
    "    rows.append({\"audio_path\": wav, \"video_path\": vp, \"label\": label})\n",
    "\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    import pandas as pd\n",
    "    pd.DataFrame(rows).to_csv(csv_path, index=False)\n",
    "    print(f\"CSV creato: {csv_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec91ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LID:   0%|          | 0/95 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "incorrect audio shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# 5) Passa a mel e al modello\u001b[39;00m\n\u001b[1;32m     79\u001b[0m mel \u001b[38;5;241m=\u001b[39m whisper\u001b[38;5;241m.\u001b[39mlog_mel_spectrogram(audio)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 80\u001b[0m _, probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdetect_language(mel)\n\u001b[1;32m     81\u001b[0m pred_lang \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(probs, key\u001b[38;5;241m=\u001b[39mprobs\u001b[38;5;241m.\u001b[39mget)\n\u001b[1;32m     83\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(pred_lang)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/whisper/decoding.py:52\u001b[0m, in \u001b[0;36mdetect_language\u001b[0;34m(model, mel, tokenizer)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# skip encoder forward pass if already-encoded audio features were given\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m!=\u001b[39m (model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_ctx, model\u001b[38;5;241m.\u001b[39mdims\u001b[38;5;241m.\u001b[39mn_audio_state):\n\u001b[0;32m---> 52\u001b[0m     mel \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoder(mel)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# forward pass using a single token, startoftranscript\u001b[39;00m\n\u001b[1;32m     55\u001b[0m n_audio \u001b[38;5;241m=\u001b[39m mel\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/whisper/model.py:197\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    194\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m    195\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincorrect audio shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "\u001b[0;31mAssertionError\u001b[0m: incorrect audio shape"
     ]
    }
   ],
   "source": [
    "import whisper, pandas as pd, torch, soundfile as sf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import webrtcvad\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "MODEL_NAME   = \"small\"\n",
    "CSV          = \"/Users/ludovicagenovese/.../dataset.csv\"\n",
    "id2lang      = {0: \"en\", 1: \"it\", 2: \"es\"}\n",
    "SR           = 16000\n",
    "MAX_SECONDS  = 10         # 10 secondi di parlato\n",
    "MAX_SAMPLES  = MAX_SECONDS * SR\n",
    "vad          = webrtcvad.Vad(1)\n",
    "\n",
    "# 1) estrai l’audio “speech-only” come numpy array (float32, mono)\n",
    "def extract_speech_np(audio: np.ndarray, sr: int, vad: webrtcvad.Vad, max_samples: int) -> np.ndarray:\n",
    "    frame_ms     = 30\n",
    "    frame_len    = int(sr * frame_ms / 1000)\n",
    "    audio_int16  = (audio * 32767).astype(np.int16)\n",
    "    speech_bytes = bytearray()\n",
    "    speech_on    = False\n",
    "\n",
    "    for i in range(0, len(audio_int16), frame_len):\n",
    "        chunk = audio_int16[i:i+frame_len]\n",
    "        if len(chunk) < frame_len:\n",
    "            break\n",
    "        b = chunk.tobytes()\n",
    "        if vad.is_speech(b, sr):\n",
    "            speech_on = True\n",
    "            speech_bytes += b\n",
    "        elif speech_on:\n",
    "            speech_bytes += b\n",
    "        if len(speech_bytes) >= max_samples * 2:\n",
    "            break\n",
    "\n",
    "    speech = np.frombuffer(speech_bytes, dtype=np.int16).astype(np.float32) / 32767\n",
    "    return speech\n",
    "\n",
    "# carica modello e dataset\n",
    "model  = whisper.load_model(MODEL_NAME).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "df     = pd.read_csv(CSV, usecols=[\"audio_path\", \"label\"])\n",
    "df[\"label\"] = df[\"label\"].map(id2lang)\n",
    "\n",
    "preds, golds = [], []\n",
    "for wav, true_lang in tqdm(df.itertuples(index=False), total=len(df), desc=\"LID\"):\n",
    "    audio, sr = sf.read(wav)\n",
    "    if sr != SR:\n",
    "        raise ValueError(f\"{wav}: campionamento ≠16 kHz\")\n",
    "\n",
    "    # 2) mono e float32\n",
    "    if audio.ndim > 1:\n",
    "        audio = audio.mean(axis=1)\n",
    "    audio = audio.astype(np.float32)\n",
    "\n",
    "    # 3) VAD → numpy speech-only\n",
    "    speech_np = extract_speech_np(audio, SR, vad, MAX_SAMPLES)\n",
    "\n",
    "    # 4) torch.Tensor 1D + pad/trim manuale\n",
    "    x = torch.from_numpy(speech_np)\n",
    "    if x.dim() > 1:\n",
    "        x = x.squeeze()\n",
    "    if x.numel() < MAX_SAMPLES:\n",
    "        x = F.pad(x, (0, MAX_SAMPLES - x.numel()))\n",
    "    else:\n",
    "        x = x[:MAX_SAMPLES]\n",
    "\n",
    "    # 5) mel + detect_language\n",
    "    mel       = whisper.log_mel_spectrogram(x).to(model.device)\n",
    "    _, probs  = model.detect_language(mel)\n",
    "    pred_lang = max(probs, key=probs.get)\n",
    "\n",
    "    preds.append(pred_lang)\n",
    "    golds.append(true_lang)\n",
    "\n",
    "# report finale\n",
    "labels = [\"en\",\"it\",\"es\"]\n",
    "print(classification_report(golds, preds, labels=labels, target_names=labels, digits=3))\n",
    "print(\"Confusion matrix\\n\", confusion_matrix(golds, preds, labels=labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e8de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 95/95 [01:37<00:00,  1.02s/it]\n"
     ]
    }
   ],
   "source": [
    "#estrazione delle feature dal teacher\n",
    "import whisper, torch, soundfile as sf, json, numpy as np, pandas as pd\n",
    "from tqdm import tqdm\n",
    "import webrtcvad\n",
    "\n",
    "dataset_dir  = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/dataset.csv\"                 \n",
    "out_dir = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/distillation_dataset.csv\"\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "device = model.device\n",
    "id2lang = {0: \"en\", 1: \"it\", 2: \"es\"}\n",
    "\n",
    "\n",
    "\n",
    "vad = webrtcvad.Vad(3)   # 0–3 aggressività\n",
    "\n",
    "def frame_generator(frame_duration_ms, audio, sample_rate):\n",
    "    n = int(sample_rate * (frame_duration_ms / 1000.0))\n",
    "    offset = 0\n",
    "    while offset + n < len(audio):\n",
    "        yield audio[offset:offset + n]\n",
    "        offset += n\n",
    "# num di secondi presi in considerazione - per adesso cambia son 10\n",
    "def vad_collector(audio, sample_rate, max_speech_s=10):\n",
    "    \n",
    "    frames = list(frame_generator(10, audio, sample_rate))\n",
    "    speech_segments, curr_start, speech_time = [], None, 0.0\n",
    "    t0 = 0.0\n",
    "    for f in frames:\n",
    "        is_speech = vad.is_speech((f*32768).astype('int16').tobytes(), sample_rate)\n",
    "        if is_speech:\n",
    "            if curr_start is None:\n",
    "                curr_start = t0\n",
    "        else:\n",
    "            if curr_start is not None:\n",
    "                seg_dur = t0 - curr_start\n",
    "                speech_segments.append((curr_start, t0))\n",
    "                speech_time += seg_dur\n",
    "                if speech_time >= max_speech_s:\n",
    "                    break\n",
    "                curr_start = None\n",
    "        t0 += 0.03\n",
    "    # se termina parlando\n",
    "    if curr_start is not None and speech_time < max_speech_s:\n",
    "        speech_segments.append((curr_start, curr_start + min(30 - speech_time, t0 - curr_start)))\n",
    "    # tronca all’esatto 30 s se serve\n",
    "    total = 0.0\n",
    "    out = []\n",
    "    for s,e in speech_segments:\n",
    "        dur = e-s\n",
    "        if total+dur <= max_speech_s:\n",
    "            out.append((s,e))\n",
    "            total += dur\n",
    "        else:\n",
    "            out.append((s, s + (max_speech_s-total)))\n",
    "            break\n",
    "    return out  # list of (start_s, end_s)\n",
    "\n",
    "\n",
    "rows = []\n",
    "for vid, wav, lbl in tqdm(\n",
    "        pd.read_csv(dataset_dir)[[\"video_path\", \"audio_path\", \"label\"]].values):\n",
    "\n",
    "    audio, sr = sf.read(wav)                 \n",
    "    if sr != 16000: # whisper vuole i 16 hkz\n",
    "        raise ValueError(f\"{wav}: sample-rate ≠ 16 kHz\")\n",
    "    if audio.ndim == 2:                      #Se l’audio ha due canali, ne fa la media per ottenere un vettore 1-D.\n",
    "        audio = audio.mean(axis=1)\n",
    "\n",
    "    audio = torch.from_numpy(audio).to(device).float()\n",
    "\n",
    "    # primi 30 secondi di solo parlato\n",
    "    raw = audio.cpu().numpy()        \n",
    "    segs = vad_collector(raw, sr, max_speech_s=10.0)\n",
    "\n",
    "    clipped = np.concatenate([raw[int(s*sr):int(e*sr)] for s,e in segs])\n",
    "    audio   = torch.from_numpy(clipped).to(device).float()   \n",
    "                  \n",
    "    # roba di tensori e di come tradformarli nella forma che vuole whisper \n",
    "    mel   = whisper.log_mel_spectrogram(audio)          \n",
    "    mel   = mel.to(device).float()                      \n",
    "\n",
    "\n",
    "    mel = mel.unsqueeze(0)  \n",
    "                            \n",
    "    #detect della lingua\n",
    "    with torch.no_grad():\n",
    "        enc   = model.encoder(mel)                      \n",
    "        probs = model.detect_language(mel)[1]\n",
    "\n",
    "    emb = enc.mean(1).squeeze(0).cpu().numpy()\n",
    "    # salva le soft labels\n",
    "    hid_path   = wav + \".hid.npy\"\n",
    "    probs_path = wav + \".probs.json\"\n",
    "    np.save(hid_path, emb)\n",
    "    json.dump(probs, open(probs_path, \"w\"))\n",
    "\n",
    "    rows.append([vid, wav, json.dumps(segs), probs_path, hid_path, id2lang[lbl]])\n",
    "\n",
    "\n",
    "pd.DataFrame(rows,\n",
    "    columns=[\"video_path\",\"audio_path\",\"segments\",\"probs_path\",\"hid_path\",\"label\"]\n",
    ").to_csv(out_dir, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34be0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video:   0%|          | 0/95 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "▶️ Processing video [1/95]: /Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/Italian/1_1_2_21_25.mp4\n",
      "  Segment 1/17 → frames 29–31\n",
      "    Estratto frame 29\n",
      "    Estratto frame 30\n",
      "  Segment 2/17 → frames 45–154\n",
      "    Estratto frame 45\n",
      "    Estratto frame 46\n",
      "    Estratto frame 47\n",
      "    Estratto frame 48\n",
      "    Estratto frame 49\n",
      "    Estratto frame 50\n",
      "    Estratto frame 51\n",
      "    Estratto frame 52\n",
      "    Estratto frame 53\n",
      "    Estratto frame 54\n",
      "    Estratto frame 55\n",
      "    Estratto frame 56\n",
      "    Estratto frame 57\n",
      "    Estratto frame 58\n",
      "    Estratto frame 59\n",
      "    Estratto frame 60\n",
      "    Estratto frame 61\n",
      "    Estratto frame 62\n",
      "    Estratto frame 63\n",
      "    Estratto frame 64\n",
      "    Estratto frame 65\n",
      "    Estratto frame 66\n",
      "    Estratto frame 67\n",
      "    Estratto frame 68\n",
      "    Estratto frame 69\n",
      "    Estratto frame 70\n",
      "    Estratto frame 71\n",
      "    Estratto frame 72\n",
      "    Estratto frame 73\n",
      "    Estratto frame 74\n",
      "    Estratto frame 75\n",
      "    Estratto frame 76\n",
      "    Estratto frame 77\n",
      "    Estratto frame 78\n",
      "    Estratto frame 79\n",
      "    Estratto frame 80\n",
      "    Estratto frame 81\n",
      "    Estratto frame 82\n",
      "    Estratto frame 83\n",
      "    Estratto frame 84\n",
      "    Estratto frame 85\n",
      "    Estratto frame 86\n",
      "    Estratto frame 87\n",
      "    Estratto frame 88\n",
      "    Estratto frame 89\n",
      "    Estratto frame 90\n",
      "    Estratto frame 91\n",
      "    Estratto frame 92\n",
      "    Estratto frame 93\n",
      "    Estratto frame 94\n",
      "    Estratto frame 95\n",
      "    Estratto frame 96\n",
      "    Estratto frame 97\n",
      "    Estratto frame 98\n",
      "    Estratto frame 99\n",
      "    Estratto frame 100\n",
      "    Estratto frame 101\n",
      "    Estratto frame 102\n",
      "    Estratto frame 103\n",
      "    Estratto frame 104\n",
      "    Estratto frame 105\n",
      "    Estratto frame 106\n",
      "    Estratto frame 107\n",
      "    Estratto frame 108\n",
      "    Estratto frame 109\n",
      "    Estratto frame 110\n",
      "    Estratto frame 111\n",
      "    Estratto frame 112\n",
      "    Estratto frame 113\n",
      "    Estratto frame 114\n",
      "    Estratto frame 115\n",
      "    Estratto frame 116\n",
      "    Estratto frame 117\n",
      "    Estratto frame 118\n",
      "    Estratto frame 119\n",
      "    Estratto frame 120\n",
      "    Estratto frame 121\n",
      "    Estratto frame 122\n",
      "    Estratto frame 123\n",
      "    Estratto frame 124\n",
      "    Estratto frame 125\n",
      "    Estratto frame 126\n",
      "    Estratto frame 127\n",
      "    Estratto frame 128\n",
      "    Estratto frame 129\n",
      "    Estratto frame 130\n",
      "    Estratto frame 131\n",
      "    Estratto frame 132\n",
      "    Estratto frame 133\n",
      "    Estratto frame 134\n",
      "    Estratto frame 135\n",
      "    Estratto frame 136\n",
      "    Estratto frame 137\n",
      "    Estratto frame 138\n",
      "    Estratto frame 139\n",
      "    Estratto frame 140\n",
      "    Estratto frame 141\n",
      "    Estratto frame 142\n",
      "    Estratto frame 143\n",
      "    Estratto frame 144\n",
      "    Estratto frame 145\n",
      "    Estratto frame 146\n",
      "    Estratto frame 147\n",
      "    Estratto frame 148\n",
      "    Estratto frame 149\n",
      "    Estratto frame 150\n",
      "    Estratto frame 151\n",
      "    Estratto frame 152\n",
      "    Estratto frame 153\n",
      "  Segment 3/17 → frames 156–189\n",
      "    Estratto frame 156\n",
      "    Estratto frame 157\n",
      "    Estratto frame 158\n",
      "    Estratto frame 159\n",
      "    Estratto frame 160\n",
      "    Estratto frame 161\n",
      "    Estratto frame 162\n",
      "    Estratto frame 163\n",
      "    Estratto frame 164\n",
      "    Estratto frame 165\n",
      "    Estratto frame 166\n",
      "    Estratto frame 167\n",
      "    Estratto frame 168\n",
      "    Estratto frame 169\n",
      "    Estratto frame 170\n",
      "    Estratto frame 171\n",
      "    Estratto frame 172\n",
      "    Estratto frame 173\n",
      "    Estratto frame 174\n",
      "    Estratto frame 175\n",
      "    Estratto frame 176\n",
      "    Estratto frame 177\n",
      "    Estratto frame 178\n",
      "    Estratto frame 179\n",
      "    Estratto frame 180\n",
      "    Estratto frame 181\n",
      "    Estratto frame 182\n",
      "    Estratto frame 183\n",
      "    Estratto frame 184\n",
      "    Estratto frame 185\n",
      "    Estratto frame 186\n",
      "    Estratto frame 187\n",
      "    Estratto frame 188\n",
      "  Segment 4/17 → frames 189–192\n",
      "    Estratto frame 189\n",
      "    Estratto frame 190\n",
      "    Estratto frame 191\n",
      "  Segment 5/17 → frames 213–260\n",
      "    Estratto frame 213\n",
      "    Estratto frame 214\n",
      "    Estratto frame 215\n",
      "    Estratto frame 216\n",
      "    Estratto frame 217\n",
      "    Estratto frame 218\n",
      "    Estratto frame 219\n",
      "    Estratto frame 220\n",
      "    Estratto frame 221\n",
      "    Estratto frame 222\n",
      "    Estratto frame 223\n",
      "    Estratto frame 224\n",
      "    Estratto frame 225\n",
      "    Estratto frame 226\n",
      "    Estratto frame 227\n",
      "    Estratto frame 228\n",
      "    Estratto frame 229\n",
      "    Estratto frame 230\n",
      "    Estratto frame 231\n",
      "    Estratto frame 232\n",
      "    Estratto frame 233\n",
      "    Estratto frame 234\n",
      "    Estratto frame 235\n",
      "    Estratto frame 236\n",
      "    Estratto frame 237\n",
      "    Estratto frame 238\n",
      "    Estratto frame 239\n",
      "    Estratto frame 240\n",
      "    Estratto frame 241\n",
      "    Estratto frame 242\n",
      "    Estratto frame 243\n",
      "    Estratto frame 244\n",
      "    Estratto frame 245\n",
      "    Estratto frame 246\n",
      "    Estratto frame 247\n",
      "    Estratto frame 248\n",
      "    Estratto frame 249\n",
      "    Estratto frame 250\n",
      "    Estratto frame 251\n",
      "    Estratto frame 252\n",
      "    Estratto frame 253\n",
      "    Estratto frame 254\n",
      "    Estratto frame 255\n",
      "    Estratto frame 256\n",
      "    Estratto frame 257\n",
      "    Estratto frame 258\n",
      "    Estratto frame 259\n",
      "  Segment 6/17 → frames 261–370\n",
      "    Estratto frame 261\n",
      "    Estratto frame 262\n",
      "    Estratto frame 263\n",
      "    Estratto frame 264\n",
      "    Estratto frame 265\n",
      "    Estratto frame 266\n",
      "    Estratto frame 267\n",
      "    Estratto frame 268\n",
      "    Estratto frame 269\n",
      "    Estratto frame 270\n",
      "    Estratto frame 271\n",
      "    Estratto frame 272\n",
      "    Estratto frame 273\n",
      "    Estratto frame 274\n",
      "    Estratto frame 275\n",
      "    Estratto frame 276\n",
      "    Estratto frame 277\n",
      "    Estratto frame 278\n",
      "    Estratto frame 279\n",
      "    Estratto frame 280\n",
      "    Estratto frame 281\n",
      "    Estratto frame 282\n",
      "    Estratto frame 283\n",
      "    Estratto frame 284\n",
      "    Estratto frame 285\n",
      "    Estratto frame 286\n",
      "    Estratto frame 287\n",
      "    Estratto frame 288\n",
      "    Estratto frame 289\n",
      "    Estratto frame 290\n",
      "    Estratto frame 291\n",
      "    Estratto frame 292\n",
      "    Estratto frame 293\n",
      "    Estratto frame 294\n",
      "    Estratto frame 295\n",
      "    Estratto frame 296\n",
      "    Estratto frame 297\n",
      "    Estratto frame 298\n",
      "    Estratto frame 299\n",
      "    Estratto frame 300\n",
      "    Estratto frame 301\n",
      "    Estratto frame 302\n",
      "    Estratto frame 303\n",
      "    Estratto frame 304\n",
      "    Estratto frame 305\n",
      "    Estratto frame 306\n",
      "    Estratto frame 307\n",
      "    Estratto frame 308\n",
      "    Estratto frame 309\n",
      "    Estratto frame 310\n",
      "    Estratto frame 311\n",
      "    Estratto frame 312\n",
      "    Estratto frame 313\n",
      "    Estratto frame 314\n",
      "    Estratto frame 315\n",
      "    Estratto frame 316\n",
      "    Estratto frame 317\n",
      "    Estratto frame 318\n",
      "    Estratto frame 319\n",
      "    Estratto frame 320\n",
      "    Estratto frame 321\n",
      "    Estratto frame 322\n",
      "    Estratto frame 323\n",
      "    Estratto frame 324\n",
      "    Estratto frame 325\n",
      "    Estratto frame 326\n",
      "    Estratto frame 327\n",
      "    Estratto frame 328\n",
      "    Estratto frame 329\n",
      "    Estratto frame 330\n",
      "    Estratto frame 331\n",
      "    Estratto frame 332\n",
      "    Estratto frame 333\n",
      "    Estratto frame 334\n",
      "    Estratto frame 335\n",
      "    Estratto frame 336\n",
      "    Estratto frame 337\n",
      "    Estratto frame 338\n",
      "    Estratto frame 339\n",
      "    Estratto frame 340\n",
      "    Estratto frame 341\n",
      "    Estratto frame 342\n",
      "    Estratto frame 343\n",
      "    Estratto frame 344\n",
      "    Estratto frame 345\n",
      "    Estratto frame 346\n",
      "    Estratto frame 347\n",
      "    Estratto frame 348\n",
      "    Estratto frame 349\n",
      "    Estratto frame 350\n",
      "    Estratto frame 351\n",
      "    Estratto frame 352\n",
      "    Estratto frame 353\n",
      "    Estratto frame 354\n",
      "    Estratto frame 355\n",
      "    Estratto frame 356\n",
      "    Estratto frame 357\n",
      "    Estratto frame 358\n",
      "    Estratto frame 359\n",
      "    Estratto frame 360\n",
      "    Estratto frame 361\n",
      "    Estratto frame 362\n",
      "    Estratto frame 363\n",
      "    Estratto frame 364\n",
      "    Estratto frame 365\n",
      "    Estratto frame 366\n",
      "    Estratto frame 367\n",
      "    Estratto frame 368\n",
      "    Estratto frame 369\n",
      "  Segment 7/17 → frames 378–532\n",
      "    Estratto frame 378\n",
      "    Estratto frame 379\n",
      "    Estratto frame 380\n",
      "    Estratto frame 381\n",
      "    Estratto frame 382\n",
      "    Estratto frame 383\n",
      "    Estratto frame 384\n",
      "    Estratto frame 385\n",
      "    Estratto frame 386\n",
      "    Estratto frame 387\n",
      "    Estratto frame 388\n",
      "    Estratto frame 389\n",
      "    Estratto frame 390\n",
      "    Estratto frame 391\n",
      "    Estratto frame 392\n",
      "    Estratto frame 393\n",
      "    Estratto frame 394\n",
      "    Estratto frame 395\n",
      "    Estratto frame 396\n",
      "    Estratto frame 397\n",
      "    Estratto frame 398\n",
      "    Estratto frame 399\n",
      "    Estratto frame 400\n",
      "    Estratto frame 401\n",
      "    Estratto frame 402\n",
      "    Estratto frame 403\n",
      "    Estratto frame 404\n",
      "    Estratto frame 405\n",
      "    Estratto frame 406\n",
      "    Estratto frame 407\n",
      "    Estratto frame 408\n",
      "    Estratto frame 409\n",
      "    Estratto frame 410\n",
      "    Estratto frame 411\n",
      "    Estratto frame 412\n",
      "    Estratto frame 413\n",
      "    Estratto frame 414\n",
      "    Estratto frame 415\n",
      "    Estratto frame 416\n",
      "    Estratto frame 417\n",
      "    Estratto frame 418\n",
      "    Estratto frame 419\n",
      "    Estratto frame 420\n",
      "    Estratto frame 421\n",
      "    Estratto frame 422\n",
      "    Estratto frame 423\n",
      "    Estratto frame 424\n",
      "    Estratto frame 425\n",
      "    Estratto frame 426\n",
      "    Estratto frame 427\n",
      "    Estratto frame 428\n",
      "    Estratto frame 429\n",
      "    Estratto frame 430\n",
      "    Estratto frame 431\n",
      "    Estratto frame 432\n",
      "    Estratto frame 433\n",
      "    Estratto frame 434\n",
      "    Estratto frame 435\n",
      "    Estratto frame 436\n",
      "    Estratto frame 437\n",
      "    Estratto frame 438\n",
      "    Estratto frame 439\n",
      "    Estratto frame 440\n",
      "    Estratto frame 441\n",
      "    Estratto frame 442\n",
      "    Estratto frame 443\n",
      "    Estratto frame 444\n",
      "    Estratto frame 445\n",
      "    Estratto frame 446\n",
      "    Estratto frame 447\n",
      "    Estratto frame 448\n",
      "    Estratto frame 449\n",
      "    Estratto frame 450\n",
      "    Estratto frame 451\n",
      "    Estratto frame 452\n",
      "    Estratto frame 453\n",
      "    Estratto frame 454\n",
      "    Estratto frame 455\n",
      "    Estratto frame 456\n",
      "    Estratto frame 457\n",
      "    Estratto frame 458\n",
      "    Estratto frame 459\n",
      "    Estratto frame 460\n",
      "    Estratto frame 461\n",
      "    Estratto frame 462\n",
      "    Estratto frame 463\n",
      "    Estratto frame 464\n",
      "    Estratto frame 465\n",
      "    Estratto frame 466\n",
      "    Estratto frame 467\n",
      "    Estratto frame 468\n",
      "    Estratto frame 469\n",
      "    Estratto frame 470\n",
      "    Estratto frame 471\n",
      "    Estratto frame 472\n",
      "    Estratto frame 473\n",
      "    Estratto frame 474\n",
      "    Estratto frame 475\n",
      "    Estratto frame 476\n",
      "    Estratto frame 477\n",
      "    Estratto frame 478\n",
      "    Estratto frame 479\n",
      "    Estratto frame 480\n",
      "    Estratto frame 481\n",
      "    Estratto frame 482\n",
      "    Estratto frame 483\n",
      "    Estratto frame 484\n",
      "    Estratto frame 485\n",
      "    Estratto frame 486\n",
      "    Estratto frame 487\n",
      "    Estratto frame 488\n",
      "    Estratto frame 489\n",
      "    Estratto frame 490\n",
      "    Estratto frame 491\n",
      "    Estratto frame 492\n",
      "    Estratto frame 493\n",
      "    Estratto frame 494\n",
      "    Estratto frame 495\n",
      "    Estratto frame 496\n",
      "    Estratto frame 497\n",
      "    Estratto frame 498\n",
      "    Estratto frame 499\n",
      "    Estratto frame 500\n",
      "    Estratto frame 501\n",
      "    Estratto frame 502\n",
      "    Estratto frame 503\n",
      "    Estratto frame 504\n",
      "    Estratto frame 505\n",
      "    Estratto frame 506\n",
      "    Estratto frame 507\n",
      "    Estratto frame 508\n",
      "    Estratto frame 509\n",
      "    Estratto frame 510\n",
      "    Estratto frame 511\n",
      "    Estratto frame 512\n",
      "    Estratto frame 513\n",
      "    Estratto frame 514\n",
      "    Estratto frame 515\n",
      "    Estratto frame 516\n",
      "    Estratto frame 517\n",
      "    Estratto frame 518\n",
      "    Estratto frame 519\n",
      "    Estratto frame 520\n",
      "    Estratto frame 521\n",
      "    Estratto frame 522\n",
      "    Estratto frame 523\n",
      "    Estratto frame 524\n",
      "    Estratto frame 525\n",
      "    Estratto frame 526\n",
      "    Estratto frame 527\n",
      "    Estratto frame 528\n",
      "    Estratto frame 529\n",
      "    Estratto frame 530\n",
      "    Estratto frame 531\n",
      "  Segment 8/17 → frames 552–556\n",
      "    Estratto frame 552\n",
      "    Estratto frame 553\n",
      "    Estratto frame 554\n",
      "    Estratto frame 555\n",
      "  Segment 9/17 → frames 575–657\n",
      "    Estratto frame 575\n",
      "    Estratto frame 576\n",
      "    Estratto frame 577\n",
      "    Estratto frame 578\n",
      "    Estratto frame 579\n",
      "    Estratto frame 580\n",
      "    Estratto frame 581\n",
      "    Estratto frame 582\n",
      "    Estratto frame 583\n",
      "    Estratto frame 584\n",
      "    Estratto frame 585\n",
      "    Estratto frame 586\n",
      "    Estratto frame 587\n",
      "    Estratto frame 588\n",
      "    Estratto frame 589\n",
      "    Estratto frame 590\n",
      "    Estratto frame 591\n",
      "    Estratto frame 592\n",
      "    Estratto frame 593\n",
      "    Estratto frame 594\n",
      "    Estratto frame 595\n",
      "    Estratto frame 596\n",
      "    Estratto frame 597\n",
      "    Estratto frame 598\n",
      "    Estratto frame 599\n",
      "    Estratto frame 600\n",
      "    Estratto frame 601\n",
      "    Estratto frame 602\n",
      "    Estratto frame 603\n",
      "    Estratto frame 604\n",
      "    Estratto frame 605\n",
      "    Estratto frame 606\n",
      "    Estratto frame 607\n",
      "    Estratto frame 608\n",
      "    Estratto frame 609\n",
      "    Estratto frame 610\n",
      "    Estratto frame 611\n",
      "    Estratto frame 612\n",
      "    Estratto frame 613\n",
      "    Estratto frame 614\n",
      "    Estratto frame 615\n",
      "    Estratto frame 616\n",
      "    Estratto frame 617\n",
      "    Estratto frame 618\n",
      "    Estratto frame 619\n",
      "    Estratto frame 620\n",
      "    Estratto frame 621\n",
      "    Estratto frame 622\n",
      "    Estratto frame 623\n",
      "    Estratto frame 624\n",
      "    Estratto frame 625\n",
      "    Estratto frame 626\n",
      "    Estratto frame 627\n",
      "    Estratto frame 628\n",
      "    Estratto frame 629\n",
      "    Estratto frame 630\n",
      "    Estratto frame 631\n",
      "    Estratto frame 632\n",
      "    Estratto frame 633\n",
      "    Estratto frame 634\n",
      "    Estratto frame 635\n",
      "    Estratto frame 636\n",
      "    Estratto frame 637\n",
      "    Estratto frame 638\n",
      "    Estratto frame 639\n",
      "    Estratto frame 640\n",
      "    Estratto frame 641\n",
      "    Estratto frame 642\n",
      "    Estratto frame 643\n",
      "    Estratto frame 644\n",
      "    Estratto frame 645\n",
      "    Estratto frame 646\n",
      "    Estratto frame 647\n",
      "    Estratto frame 648\n",
      "    Estratto frame 649\n",
      "    Estratto frame 650\n",
      "    Estratto frame 651\n",
      "    Estratto frame 652\n",
      "    Estratto frame 653\n",
      "    Estratto frame 654\n",
      "    Estratto frame 655\n",
      "    Estratto frame 656\n",
      "  Segment 10/17 → frames 668–723\n",
      "    Estratto frame 668\n",
      "    Estratto frame 669\n",
      "    Estratto frame 670\n",
      "    Estratto frame 671\n",
      "    Estratto frame 672\n",
      "    Estratto frame 673\n",
      "    Estratto frame 674\n",
      "    Estratto frame 675\n",
      "    Estratto frame 676\n",
      "    Estratto frame 677\n",
      "    Estratto frame 678\n",
      "    Estratto frame 679\n",
      "    Estratto frame 680\n",
      "    Estratto frame 681\n",
      "    Estratto frame 682\n",
      "    Estratto frame 683\n",
      "    Estratto frame 684\n",
      "    Estratto frame 685\n",
      "    Estratto frame 686\n",
      "    Estratto frame 687\n",
      "    Estratto frame 688\n",
      "    Estratto frame 689\n",
      "    Estratto frame 690\n",
      "    Estratto frame 691\n",
      "    Estratto frame 692\n",
      "    Estratto frame 693\n",
      "    Estratto frame 694\n",
      "    Estratto frame 695\n",
      "    Estratto frame 696\n",
      "    Estratto frame 697\n",
      "    Estratto frame 698\n",
      "    Estratto frame 699\n",
      "    Estratto frame 700\n",
      "    Estratto frame 701\n",
      "    Estratto frame 702\n",
      "    Estratto frame 703\n",
      "    Estratto frame 704\n",
      "    Estratto frame 705\n",
      "    Estratto frame 706\n",
      "    Estratto frame 707\n",
      "    Estratto frame 708\n",
      "    Estratto frame 709\n",
      "    Estratto frame 710\n",
      "    Estratto frame 711\n",
      "    Estratto frame 712\n",
      "    Estratto frame 713\n",
      "    Estratto frame 714\n",
      "    Estratto frame 715\n",
      "    Estratto frame 716\n",
      "    Estratto frame 717\n",
      "    Estratto frame 718\n",
      "    Estratto frame 719\n",
      "    Estratto frame 720\n",
      "    Estratto frame 721\n",
      "    Estratto frame 722\n",
      "  Segment 11/17 → frames 729–745\n",
      "    Estratto frame 729\n",
      "    Estratto frame 730\n",
      "    Estratto frame 731\n",
      "    Estratto frame 732\n",
      "    Estratto frame 733\n",
      "    Estratto frame 734\n",
      "    Estratto frame 735\n",
      "    Estratto frame 736\n",
      "    Estratto frame 737\n",
      "    Estratto frame 738\n",
      "    Estratto frame 739\n",
      "    Estratto frame 740\n",
      "    Estratto frame 741\n",
      "    Estratto frame 742\n",
      "    Estratto frame 743\n",
      "    Estratto frame 744\n",
      "  Segment 12/17 → frames 750–753\n",
      "    Estratto frame 750\n",
      "    Estratto frame 751\n",
      "    Estratto frame 752\n",
      "  Segment 13/17 → frames 754–802\n",
      "    Estratto frame 754\n",
      "    Estratto frame 755\n",
      "    Estratto frame 756\n",
      "    Estratto frame 757\n",
      "    Estratto frame 758\n",
      "    Estratto frame 759\n",
      "    Estratto frame 760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Video:   0%|          | 0/95 [13:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    ⚠️ Unexpected end at frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m lm \u001b[38;5;241m=\u001b[39m fa\u001b[38;5;241m.\u001b[39mget_landmarks(frame)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lm) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/api.py:113\u001b[0m, in \u001b[0;36mFaceAlignment.get_landmarks\u001b[0;34m(self, image_or_path, detected_faces, return_bboxes, return_landmark_score)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_landmarks\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_or_path, detected_faces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, return_bboxes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_landmark_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deprecated, please use get_landmarks_from_image\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03m        return_landmark_score {boolean} -- If True, return the keypoint scores along with the keypoints.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_landmarks_from_image(image_or_path, detected_faces, return_bboxes, return_landmark_score)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/api.py:144\u001b[0m, in \u001b[0;36mFaceAlignment.get_landmarks_from_image\u001b[0;34m(self, image_or_path, detected_faces, return_bboxes, return_landmark_score)\u001b[0m\n\u001b[1;32m    141\u001b[0m image \u001b[38;5;241m=\u001b[39m get_image(image_or_path)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m detected_faces \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 144\u001b[0m     detected_faces \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector\u001b[38;5;241m.\u001b[39mdetect_from_image(image\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(detected_faces) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    147\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo faces were detected.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/detection/sfd/sfd_detector.py:45\u001b[0m, in \u001b[0;36mSFDDetector.detect_from_image\u001b[0;34m(self, tensor_or_path)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_from_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor_or_path):\n\u001b[1;32m     43\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensor_or_path_to_ndarray(tensor_or_path)\n\u001b[0;32m---> 45\u001b[0m     bboxlist \u001b[38;5;241m=\u001b[39m detect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mface_detector, image, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     46\u001b[0m     bboxlist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filter_bboxes(bboxlist)\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bboxlist\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/detection/sfd/detect.py:17\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(net, img, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     15\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(img\u001b[38;5;241m.\u001b[39mcopy())\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_detect(net, img, device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/detection/sfd/detect.py:36\u001b[0m, in \u001b[0;36mbatch_detect\u001b[0;34m(net, img_batch, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m img_batch \u001b[38;5;241m=\u001b[39m img_batch \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m104.0\u001b[39m, \u001b[38;5;241m117.0\u001b[39m, \u001b[38;5;241m123.0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 36\u001b[0m     olist \u001b[38;5;241m=\u001b[39m net(img_batch)  \u001b[38;5;66;03m# patched uint8_t overflow error\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(olist) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     39\u001b[0m     olist[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(olist[i \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/face_alignment/detection/sfd/net_s3fd.py:74\u001b[0m, in \u001b[0;36ms3fd.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(h, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     73\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_1(h), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 74\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2_2(h), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     75\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool2d(h, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     77\u001b[0m h \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3_1(h), inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/casaenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[1;32m    551\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import face_alignment\n",
    "from face_alignment import FaceAlignment\n",
    "\n",
    "# --- Configurazione ---\n",
    "distill_csv = \"/Users/ludovicagenovese/Documents/GitHub/mothertongueVSspoken/BABELE/distillation_dataset.csv\"\n",
    "out_dir     = \"video_mouth\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Inizializza FaceAlignment su CPU (2D landmarks di default)\n",
    "fa = FaceAlignment(face_alignment.LandmarksType.TWO_D, device=\"cpu\")\n",
    "\n",
    "def mouth_roi(frame, landmarks, size=96, scale=1.4):\n",
    "    pts = landmarks[48:68]\n",
    "    cx, cy = pts.mean(axis=0)\n",
    "    w = max(np.ptp(pts[:,0]), np.ptp(pts[:,1])) * scale\n",
    "    x1, y1 = int(cx - w/2), int(cy - w/2)\n",
    "    crop = frame[y1:y1+int(w), x1:x1+int(w)]\n",
    "    return cv2.resize(crop, (size, size))\n",
    "\n",
    "# Leggi il manifest\n",
    "mani = pd.read_csv(distill_csv)\n",
    "mouth_paths = []\n",
    "\n",
    "# Processa ogni video\n",
    "for idx, row in tqdm(mani.iterrows(), total=len(mani), desc=\"Video\"):\n",
    "    video_path = row[\"video_path\"]\n",
    "    print(f\"\\n▶️ Processing video [{idx+1}/{len(mani)}]: {video_path}\")\n",
    "    segments = json.loads(row.get(\"segments\", \"[]\"))\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 25\n",
    "    frames = []\n",
    "\n",
    "    # Scorri i segmenti parlati\n",
    "    for seg_i, (start_s, end_s) in enumerate(segments, start=1):\n",
    "        start_f = int(start_s * fps)\n",
    "        end_f   = int(end_s   * fps)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, start_f)\n",
    "        print(f\"  Segment {seg_i}/{len(segments)} → frames {start_f}–{end_f}\")\n",
    "\n",
    "        for f in range(start_f, end_f):\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                print(f\"    ⚠️ Unexpected end at frame {f}\")\n",
    "                break\n",
    "\n",
    "            lm = fa.get_landmarks(frame)\n",
    "            if lm is None or len(lm) == 0:\n",
    "                continue\n",
    "\n",
    "            roi = mouth_roi(frame, lm[0])\n",
    "            frames.append(roi)\n",
    "            print(f\"    Estratto frame {f}\")\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if not frames:\n",
    "        print(f\"⚠️ Nessun mouth ROI estratto per {video_path}\")\n",
    "        mouth_paths.append(\"\")\n",
    "        continue\n",
    "\n",
    "    arr = np.stack(frames).astype(\"uint8\")\n",
    "    out_path = os.path.join(out_dir, os.path.basename(video_path) + \".npy\")\n",
    "    np.save(out_path, arr)\n",
    "    mouth_paths.append(out_path)\n",
    "    print(f\"✅ Salvati {len(frames)} frame in {out_path}\")\n",
    "\n",
    "# Aggiorna il manifest con i percorsi dei .npy\n",
    "mani[\"mouth_path\"] = mouth_paths\n",
    "mani.to_csv(distill_csv, index=False)\n",
    "\n",
    "# Split stratificato in train/val/test\n",
    "t_train, temp = train_test_split(mani, test_size=0.30, stratify=mani.label, random_state=42)\n",
    "t_val,   t_test = train_test_split(temp,   test_size=0.50, stratify=temp.label, random_state=42)\n",
    "\n",
    "t_train.to_csv(\"dist_train.csv\", index=False)\n",
    "t_val.  to_csv(\"dist_val.csv\",   index=False)\n",
    "t_test. to_csv(\"dist_test.csv\",  index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6aa749",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.py non ho capito ma okay\n",
    "\n",
    "import torch, json, numpy as np, pandas as pd\n",
    "\n",
    "class LipKD(torch.utils.data.Dataset):\n",
    "    def __init__(self, manifest):\n",
    "        self.df = pd.read_csv(manifest)\n",
    "        self.lang2id = {\"en\":0,\"it\":1,\"es\":2}\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row   = self.df.iloc[idx]\n",
    "        video = np.load(row.mouth_path) / 255.0           # T,H,W,3\n",
    "        video = torch.from_numpy(video).permute(0,3,1,2)  # T,C,H,W\n",
    "        logits_t = torch.tensor(\n",
    "            [json.load(open(row.probs_path))[k] for k in [\"en\",\"it\",\"es\"]])\n",
    "        emb_t   = torch.from_numpy(np.load(row.hid_path)) # 1280\n",
    "        label   = self.lang2id[row.label]\n",
    "        return video.float(), logits_t.float(), emb_t.float(), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d2bc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creazione student\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "class LipStudent(nn.Module):\n",
    "    def __init__(self, d_emb=256, n_classes=3):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(      # 3D-Conv minimale\n",
    "            nn.Conv3d(3, 32, (3,5,5), (1,2,2), 1), nn.ReLU(),\n",
    "            nn.MaxPool3d((1,2,2)),\n",
    "            nn.Conv3d(32, 64, (3,3,3), 1, 1), nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((1,1,1)))\n",
    "        self.fc_emb = nn.Linear(64, d_emb)          # eᵥ\n",
    "        self.fc_cls = nn.Linear(d_emb, n_classes)   # logitsᵥ\n",
    "        # proiezioni\n",
    "        self.Pa = nn.Linear(1280, d_emb, bias=False)   # freeze dopo init\n",
    "        torch.nn.init.eye_(self.Pa.weight)             # identity trim\n",
    "    def forward(self, x, emb_t):\n",
    "        z = self.backbone(x).flatten(1)\n",
    "        e_v = self.fc_emb(z)\n",
    "        logits_v = self.fc_cls(e_v)\n",
    "        proj_a   = self.Pa(emb_t)\n",
    "        return logits_v, e_v, proj_a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc599454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_student.py\n",
    "import torch, torch.nn.functional as F, torch.optim as optim\n",
    "from dataset import LipKD\n",
    "from model   import LipStudent\n",
    "\n",
    "train_set = LipKD(\"manifest_train.csv\")\n",
    "val_set   = LipKD(\"manifest_val.csv\") \n",
    "loader    = torch.utils.data.DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "\n",
    "net = LipStudent().cuda()\n",
    "opt = optim.AdamW(net.parameters(), 3e-4)\n",
    "T, λ_KL, λ_MSE, λ_CE = 2.0, 1.0, 0.5, 0.2\n",
    "\n",
    "for epoch in range(30):\n",
    "    for vid, log_t, emb_t, y in loader:\n",
    "        vid, log_t, emb_t, y = vid.cuda(), log_t.cuda(), emb_t.cuda(), y.cuda()\n",
    "        log_s, e_v, p_a = net(vid, emb_t)\n",
    "\n",
    "        kl  = F.kl_div(F.log_softmax(log_s/T, -1),\n",
    "                       (log_t/T), reduction=\"batchmean\")\n",
    "        mse = F.mse_loss(e_v, p_a)\n",
    "        ce  = F.cross_entropy(log_s, y)\n",
    "\n",
    "        loss = λ_KL*kl + λ_MSE*mse + λ_CE*ce\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "    print(f\"epoch {epoch:02d}  loss {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_student.py\n",
    "import torch, pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from dataset import LipKD\n",
    "from model   import LipStudent\n",
    "\n",
    "LANGS = [\"en\",\"it\",\"es\"]\n",
    "labels_id = {l:i for i,l in enumerate(LANGS)}\n",
    "\n",
    "net = LipStudent().cuda().eval()\n",
    "net.load_state_dict(torch.load(\"best_student.pth\"))\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        LipKD(\"manifest_test.csv\"), batch_size=16, shuffle=False)\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for vid, _, _, lbl in test_loader:      # _ = teacher info, inutili ora\n",
    "        vid = vid.cuda()\n",
    "        logits, _, _ = net(vid, torch.zeros(len(vid),1280).cuda())\n",
    "        y_pred += logits.argmax(-1).cpu().tolist()\n",
    "        y_true += lbl.tolist()\n",
    "\n",
    "print(classification_report(\n",
    "        y_true, y_pred, target_names=LANGS, digits=3))\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
